{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, set_seed, GPT2Model, GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of tokens: 25\n",
      "Generated Text:  Alice lives in France, Paris - Alice, John lives in Germany, Berlin - John, Peter lives in Italy, Paris - Peter,\n"
     ]
    }
   ],
   "source": [
    "# Define the model name\n",
    "model_name = 'gpt2'  # 137M parameters\n",
    "\n",
    "# Redownload the model and tokenizer\n",
    "model1 = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# First class of prompt: city-country and person correspondence\n",
    "# prompt = \"alice - france,\\nalice ~ paris,\\nbob - germany,\\nbob ~ berlin,\\njohn - usa,\\njohn ~\"\n",
    "# prompt = \"Alice lives in France, Paris - Alice, John lives in Germany, Berlin - John, Peter lives in USA, Washington -\"\n",
    "\n",
    "# works even with some counterfactual prompts \n",
    "# prompt = \"alice lives in france,\\nparis - alice,\\nbob lives in germany,\\nberlin - bob,\\njohn lives in usa,\\nwashington -\"\n",
    "# prompt = \"alice lives france,\\nparis - alice,\\nbob lives germany,\\nberlin - bob,\\njohn lives USA,\\nwashington -\"\n",
    "# prompt = \"alice resides france,\\nparis - alice,\\nbob resides germany,\\nberlin - bob,\\njohn resides usa,\\nwashington -\"\n",
    "# prompt = \"alice resides in france,\\nparis - alice,\\nbob resides in germany,\\nberlin - bob,\\njohn resides in usa,\\nwashington -\"\n",
    "# prompt = \"alice works france,\\nparis - alice,\\nbob works germany,\\nberlin - bob,\\njohn works usa,\\nwashington -\"\n",
    "# prompt = \"alice loves france,\\nparis - alice,\\nbob loves germany,\\nberlin - bob,\\njohn loves usa,\\nwashington -\"\n",
    "# prompt = \"alice drinks france,\\nparis - alice,\\nbob drinks germany,\\nberlin - bob,\\njohn drinks usa,\\nwashington -\"\n",
    "# prompt = \"alice skyscraper france,\\nparis - alice,\\nbob skyscraper germany,\\nberlin - bob,\\njohn skyscraper usa,\\nwashington -\"\n",
    "\n",
    "# testing the working prompt - not working\n",
    "# prompt = \"alice is french,\\nparis - alice,\\nbob is german,\\nberlin - bob,\\njohn is american,\\nwashington -\"\n",
    "# prompt = \"alice ~ france,\\nparis - alice,\\nbob ~ germany,\\nberlin - bob,\\njohn ~ usa,\\nwashington -\"\n",
    "# prompt = \"alice is from france,\\nparis - alice,\\nbob is from germany,\\nberlin - bob,\\njohn is from usa,\\nwashington -\"\n",
    "# prompt = \"alice travels france,\\nparis - alice,\\nbob travels germany,\\nberlin - bob,\\njohn travels usa,\\nwashington -\"\n",
    "# prompt = \"alice cuts france,\\nparis - alice,\\nbob cuts germany,\\nberlin - bob,\\njohn travels cuts usa,\\nwashington -\"\n",
    "# prompt = \"alice drills france,\\nparis - alice,\\nbob drills germany,\\nberlin - bob,\\njohn drills cuts usa,\\nwashington -\"\n",
    "\n",
    "# corrupted prompt \n",
    "\n",
    "prompt = \"Alice lives in France, Paris - Alice, John lives in Germany, Berlin - John, Peter lives in Italy, Paris -\"\n",
    "\n",
    "# prompt = \"alice lives in france,\\nparis - alice,\\nbob lives in germany,\\nberlin - bob,\\njohn lives in usa,\\nbucharest -\"\n",
    "# prompt = \"alice france, paris alice, bob germany, berlin bob, john usa, washington\"\n",
    "# prompt = \"Alice is from France. The capital of France is Paris.\\nBob is from Germany. The capital of Germany is Berlin.\\nJohn is from the USA. The capital of the USA is\"\n",
    "# prompt = \"Alice is from France, and she lives in Paris.\\nBob is from Germany, and he lives in Berlin.\\nJohn is from USA\"\n",
    "\n",
    "\n",
    "# Tokenize the input prompt\n",
    "encoded_input = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "input_ids = encoded_input.input_ids\n",
    "print(f\"Length of tokens: {len(input_ids[0])}\")\n",
    "attention_mask = encoded_input.attention_mask\n",
    "\n",
    "# Generate text\n",
    "gen_tokens = model1.generate(\n",
    "    input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    do_sample=True,\n",
    "    temperature=0.01,\n",
    "    max_length=input_ids.size(1) + 2,\n",
    ")\n",
    "\n",
    "# Decode the generated tokens to text\n",
    "gen_text = tokenizer.batch_decode(gen_tokens, skip_special_tokens=True)[0]\n",
    "\n",
    "print(\"Generated Text: \", gen_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model2 = GPT2LMHeadModel.from_pretrained(\"openai-community/gpt2\")\n",
    "\n",
    "text = \"alice france, alice paris, bob germany, bob berlin, john usa, john \"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model2(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Jupiter is the 5th planet from the Sun.\\nSaturn is the 6th planet from the Sun.\\nUranus is the 7th planet from the Sun.\\nNeptune is the 8th planet from the Sun.\\nMercury is the 1st planet from the Sun.\\nVenus is the 2nd planet from'}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = pipeline('text-generation', model='gpt2')\n",
    "set_seed(42)\n",
    "generator(\"Jupiter is the 5th planet from the Sun.\\nSaturn is the 6th planet from the Sun.\\nUranus is the 7th planet from the Sun.\\nNeptune is the 8th planet from the Sun.\\nMercury is the 1st planet from the Sun.\\nVenus is\", max_length=70, num_return_sequences=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mech-interp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
